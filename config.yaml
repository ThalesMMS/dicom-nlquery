llm:
  provider: "ollama"
  base_url: "http://127.0.0.1:11434"
  model: "llama3.2:latest"
  temperature: 0
  timeout: 60

guardrails:
  study_date_range_default_days: 180
  max_studies_scanned_default: 700
  search_timeout_seconds: 120

search_pipeline:
  enabled: true
  structured_first: true
  max_attempts: 12
  max_rewrites: 10
  series_probe_enabled: false
  series_probe_limit: 50
  server_limit_studies: 500
  server_limit_series: 2000
  wildcard_modes: ["contains", "token_chain", "startswith"]
  telemetry:
    enabled: true

mcp:
  tool_timeout_seconds: 30
  retry:
    max_attempts: 3
    backoff_seconds: [0.5, 1.0, 2.0]

lexicon:
  path: "configs/lexicon.en-US.yaml"

rag:
  enable: false
  index_path: "../pacs-rag/data/pacs_terms.sqlite"
  top_k: 10
  min_score: 0.2
  provider: "hash"
  embed_dim: 64

ranking:
  enabled: true
  text_match_weight: 0.7
  recency_weight: 0.3

mcp:
  command: "dicom-mcp"
  config_path: "../dicom-mcp/configuration.yaml"

resolver:
  enabled: true
  require_confirmation: true
  confirmation:
    accept_tokens: ["yes", "y"]
    reject_tokens: ["no", "n"]
    prompt_template: |
      Mode: {mode}
      Source: {source_node}
      Destination: {destination_node}
      Filters:
      {filters}
      Confirm? ({accept_tokens}/{reject_tokens})
    invalid_response: "Invalid response. Use: {accept_tokens} or {reject_tokens}."
    correction_prompt: "Enter the corrected query:"
    cancel_message: "Operation cancelled."
    max_invalid_responses: 2
    max_rejections: 2
